# PPO Trainer Configuration for Pendulum-v1
# This configuration uses the new configurable trainer system

defaults:
  
  - env@training_env: transformed_env
  - env@batched_env: batched_env
  
  - model: tanh_normal
  - model@models.policy_model: tanh_normal
  - model@models.value_model: value
  
  - network: mlp
  - network@networks.policy_network: mlp
  - network@networks.value_network: mlp
  
  - collector: multi_async

  - replay_buffer: base
  - storage: lazy_tensor
  - sampler: without_replacement
  - writer: round_robin
  - trainer: ppo
  - optimizer: adam
  - loss: ppo
  - logger: wandb
  - _self_

# Network configurations
networks:
  policy_network:
    out_features: 2  # Pendulum action space is 1-dimensional
    in_features: 3   # Pendulum observation space is 3-dimensional
    num_cells: [128, 128]

  value_network:
    out_features: 1  # Value output
    in_features: 3   # Pendulum observation space
    num_cells: [128, 128]

# Model configurations
models:
  policy_model:
    return_log_prob: true
    in_keys: ["observation"]
    param_keys: ["loc", "scale"]
    out_keys: ["action"]
    network: ${networks.policy_network}

  value_model:
    in_keys: ["observation"]
    out_keys: ["state_value"]
    network: ${networks.value_network}

# Environment configuration
training_env:
  base_env:
    env_name: Pendulum-v1
    _target_: torchrl.trainers.algorithms.configs.envs.make_env
    _partial_: true
  transform:
    noops: 30
    random: true
    _target_: torchrl.trainers.algorithms.configs.transforms.make_noop_reset_env
  _partial_: true

batched_env:
  num_workers: 16
  create_env_fn: ${training_env}
  device: cpu

# Loss configuration
loss:
  actor_network: ${models.policy_model}
  critic_network: ${models.value_model}
  entropy_coeff: 0.01

# Optimizer configuration
optimizer:
  lr: 0.001

# Collector configuration
collector:
  create_env_fn: ${training_env}
  policy: ${models.policy_model}
  total_frames: 1_000_000
  frames_per_batch: 1024
  num_workers: 2

# Storage configuration
storage:
  max_size: 1024
  device: cpu
  ndim: 1

sampler:
  drop_last: true
  shuffle: true

# Replay buffer configuration
replay_buffer:
  storage: ${storage}
  sampler: ${sampler}
  writer: ${writer}
  batch_size: 128

logger:
  exp_name: ppo_pendulum_v1
  offline: false
  project: torchrl-sota-implementations

# Trainer configuration
trainer:
  collector: ${collector}
  optimizer: ${optimizer}
  replay_buffer: ${replay_buffer}
  loss_module: ${loss}
  logger: ${logger}
  total_frames: 1_000_000
  frame_skip: 1
  clip_grad_norm: true
  clip_norm: 100.0
  progress_bar: true
  seed: 42
  save_trainer_interval: 100
  log_interval: 100
  save_trainer_file: null
  optim_steps_per_batch: null
  num_epochs: 2
